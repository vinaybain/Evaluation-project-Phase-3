{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4tko2qogENs"
   },
   "source": [
    "# Insurance Claim Fraud Detection\n",
    "\n",
    "**Project Description**\n",
    "\n",
    "Insurance fraud is a huge problem in the industry. It's difficult to identify fraud claims. Machine Learning is in a unique position to help the Auto Insurance industry with this problem.\n",
    "\n",
    "In this project, you are provided a dataset which has the details of the insurance policy along with the customer details. It also has the details of the accident on the basis of which the claims have been made.\n",
    "In this example, you will be working with some auto insurance data to demonstrate how you can create a predictive model that predicts if an insurance claim is fraudulent or not.\n",
    "\n",
    "\n",
    "**Independent Variables**\n",
    "\n",
    "1.\tmonths_as_customer: Number of months of patronage\n",
    "\n",
    "2.\tage: the length of time a customer has lived or a thing has existed\n",
    "\n",
    "3.\tpolicy_number: It is a unique id given to the customer, to track the subscription status and other details of customer\n",
    "\n",
    "4.\tpolicy_bind_date:date which document that is given to customer after we accept your proposal for insurance\n",
    "\n",
    "5.\tpolicy_state: This identifies who is the insured, what risks or property are covered, the policy limits, and the policy period\n",
    "\n",
    "6.\tpolicy_csl: is basically Combined Single Limit\n",
    "\n",
    "7.\tpolicy_deductable: the amount of money that a customer is responsible for paying toward an insured loss\n",
    "\n",
    "8.\tpolicy_annual_premium: This means the amount of Regular Premium payable by the Policyholder in a Policy Year\n",
    "\n",
    "9.\tumbrella_limit: This means extra insurance that provides protection beyond existing limits and coverages of other policies\n",
    "\n",
    "10.\tinsured_zip: It is the zip code where the insurance was made\n",
    "\n",
    "11.\tinsured_sex: This refres to either of the two main categories (male and female) into which customer are divided on the basis of their reproductive functions\n",
    "\n",
    "12.\tinsured_education_level: This refers to the Level of education of the customer\n",
    "\n",
    "13.\tinsured_occupation: This refers Occupation of the customer\n",
    "\n",
    "14.\tinsured_hobbies: This refers to an activity done regularly by customer in his/her leisure time for pleasure.\n",
    "\n",
    "15.\tinsured_relationship: This whether customer is: single; or. married; or. in a de facto relationship (that is, living together but not married); or. in a civil partnership\n",
    "\n",
    "16.\tcapital-gains: This refers to profit accrued due to insurance premium\n",
    "\n",
    "17.\tcapital-loss: This refers to the losses incurred due to insurance claims\n",
    "\n",
    "18.\tincident_date: This refers to the date which claims where made by customers\n",
    "\n",
    "19.\tincident_type: This refers to the type of claim/vehicle damage made by customer\n",
    "\n",
    "20.\tcollision_type: This refers to the area of damage on the vehicle\n",
    "\n",
    "21.\tincident_severity: This refers to the extent/level of damage\n",
    "\n",
    "22.\tauthorities_contacted: This refers to the government agencies that were contacted after damage\n",
    "\n",
    "23.\tincident_state: This refers to the state at which the accident happened\n",
    "\n",
    "24.\tincident_city: This refers to the city at which the accident happened\n",
    "\n",
    "25.\t1ncident_location: This refers to the location at which the accident happened\n",
    "\n",
    "26.\tincident_hour_of_the_day: The period of the day which accident took place\n",
    "\n",
    "27.\tnumber_of_vehicles_involved: This refers to number of vehicles involved the accident\n",
    "\n",
    "28.\tproperty_damage: This refers to whether property was damaged or not\n",
    "\n",
    "29.\tbodily_injuries: This refers to injuries sustained\n",
    "\n",
    "30.\twitnesses: This refers to the number of witnesses involved\n",
    "\n",
    "31.\tpolice_report_available: This refers to whether the report on damage was documented or not\n",
    "\n",
    "32.\ttotal_claim_amount: This refers to the financial implications involved in claims\n",
    "\n",
    "33.\tinjury_claim: This refers to physical injuries sustained\n",
    "\n",
    "34.\tproperty_claim: This refers to property damages during incident\n",
    "\n",
    "35.\tvehicle_claim: This refers to property damages during incident\n",
    "\n",
    "36.\tauto_make: This refers to the make of the vehicle\n",
    "\n",
    "37.\tauto_model: This refers to the model of the vehicle\n",
    "\n",
    "38.\tauto_year: This refers to the year which the vehicle was manufactured\n",
    "\n",
    "39.\tfraud_reported\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5IP9u0RiSL5"
   },
   "source": [
    "\n",
    "**Dataset Link-**\n",
    "\n",
    "\n",
    "â€¢\thttps://raw.githubusercontent.com/FlipRoboTechnologies/ML_-Datasets/main/Insurance%20Claim%20Fraud%20Detection/Automobile_insurance_fraud.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyFZXW-tiv7X"
   },
   "source": [
    "First Step ,we are going to start with **Data Preparation**\n",
    "\n",
    "- To load the dataset using above URl\n",
    "- To handel missing values\n",
    "- To encode categoricl variavles\n",
    "- To Siplit the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "J42EWuoeitbm"
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m dataset_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/FlipRoboTechnologies/ML_-Datasets/main/Insurance\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m20Claim\u001b[39m\u001b[38;5;132;01m%20F\u001b[39;00m\u001b[38;5;124mraud\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m20Detection/Automobile_insurance_fraud.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 10\u001b[0m df_ICFD \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(dataset_url)\n\u001b[0;32m     11\u001b[0m df_ICFD\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:713\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    710\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[0;32m    712\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[1;32m--> 713\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m _get_filepath_or_buffer(\n\u001b[0;32m    714\u001b[0m     path_or_buf,\n\u001b[0;32m    715\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m    716\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    717\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    718\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    719\u001b[0m )\n\u001b[0;32m    721\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[0;32m    722\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:363\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[0;32m    362\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urlopen(req_info) \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[0;32m    364\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    366\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:265\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m meth(req, response)\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m, request, response, code, msg, hdrs)\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "#To load the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dataset_url = 'https://raw.githubusercontent.com/FlipRoboTechnologies/ML_-Datasets/main/Insurance%20Claim%20Fraud%20Detection/Automobile_insurance_fraud.csv'\n",
    "df_ICFD = pd.read_csv(dataset_url)\n",
    "df_ICFD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gnjhzte2pzrA"
   },
   "outputs": [],
   "source": [
    "#In above data I didn't see headers part  and I need to add header column\n",
    "\n",
    "headers_col = [\n",
    "    'months_as_customer', 'age', 'policy_number', 'policy_bind_date', 'policy_state','policy_csl', 'policy_deductable', 'policy_annual_premium', 'umbrella_limit', 'insured_zip',\n",
    "    'insured_sex', 'insured_education_level', 'insured_occupation', 'insured_hobbies', 'insured_relationship','capital_gains', 'capital_loss', 'incident_date', 'incident_type', 'collision_type',\n",
    "    'incident_severity', 'authorities_contacted', 'incident_state', 'incident_city', 'incident_location','incident_hour_of_the_day', 'number_of_vehicles_involved', 'property_damage', 'bodily_injuries', 'witnesses',\n",
    "    'police_report_available', 'total_claim_amount', 'injury_claim', 'property_claim', 'vehicle_claim','auto_make', 'auto_model', 'auto_year', 'fraud_reported'\n",
    "]\n",
    "df_ICFD.columns = headers_col\n",
    "df_ICFD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZTm_zg6qhuZ"
   },
   "outputs": [],
   "source": [
    "df_ICFD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-QUQmUzwSjC"
   },
   "outputs": [],
   "source": [
    "df_ICFD.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DajSsNViw80E"
   },
   "outputs": [],
   "source": [
    "#Find any missing values\n",
    "df_ICFD.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fp7kjkSBxcYr"
   },
   "outputs": [],
   "source": [
    "df_ICFD['authorities_contacted'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jqmWPecsy9IN"
   },
   "outputs": [],
   "source": [
    "df_ICFD['authorities_contacted'].fillna(df_ICFD['authorities_contacted'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDEL9yi6zepb"
   },
   "source": [
    "we used mode method to manage the missing values in  '**authorities_contacted**'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBw_RkBGzqgV"
   },
   "outputs": [],
   "source": [
    "df_ICFD.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtvVNLK3KNEQ"
   },
   "outputs": [],
   "source": [
    "df_ICFD.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lS_uPKSMLWfa"
   },
   "outputs": [],
   "source": [
    "#Categorical Columns Unique values encoding\n",
    "\n",
    "df_ICFD['policy_state'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4PLLLUtzSGi"
   },
   "outputs": [],
   "source": [
    "df_ICFD['authorities_contacted'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MpBmEpwozYVK"
   },
   "outputs": [],
   "source": [
    "#Find any missing values\n",
    "df_ICFD.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fD2SA0cH0HRd"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRhHgbl4z8r2"
   },
   "outputs": [],
   "source": [
    "# convert date columns like 'policy_bind_date '  and 'incident_date'\n",
    "df_ICFD['policy_bind_date'] = pd.to_datetime(df_ICFD['policy_bind_date'], format ='%d-%m-%Y')\n",
    "df_ICFD['incident_date'] = pd.to_datetime(df_ICFD['incident_date'],format ='%d-%m-%Y')\n",
    "\n",
    "# Extract year and month ,day as new features\n",
    "\n",
    "df_ICFD['policy_bind_year'] = df_ICFD['policy_bind_date'].dt.year\n",
    "df_ICFD['policy_bind_month'] = df_ICFD['policy_bind_date'].dt.month\n",
    "df_ICFD['policy_bind_day'] = df_ICFD['policy_bind_date'].dt.day\n",
    "df_ICFD['incident_year'] = df_ICFD['incident_date'].dt.year\n",
    "df_ICFD['incident_month'] = df_ICFD['incident_date'].dt.month\n",
    "df_ICFD['incident_day'] = df_ICFD['incident_date'].dt.day\n",
    "\n",
    "df_ICFD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qnh0YiJH1b6W"
   },
   "outputs": [],
   "source": [
    "df_ICFD.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PM9mydTC1k7s"
   },
   "outputs": [],
   "source": [
    "# Drop policy_bind_date and incident_date\n",
    "df_ICFD.drop(['policy_bind_date', 'incident_date'], axis=1, inplace=True)\n",
    "df_ICFD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIl4pTAdFAnR"
   },
   "outputs": [],
   "source": [
    "print(df_ICFD.columns ,'\\n')\n",
    "\n",
    "df_objectes_list =df_ICFD.dtypes[df_ICFD.dtypes == 'object']\n",
    "df_objectes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9C3BgtMeFq7x"
   },
   "outputs": [],
   "source": [
    "for i in range(len(df_objectes_list.index)):\n",
    "  print( df_ICFD[df_objectes_list.index[i]].value_counts(), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwS-WVamMq9D"
   },
   "outputs": [],
   "source": [
    "\"\"\"#policy_state               category\n",
    "policy_csl                 category\n",
    "insured_sex                category\n",
    "insured_education_level    object\n",
    "insured_occupation         object\n",
    "insured_hobbies            object\n",
    "insured_relationship       object\n",
    "incident_type              object\n",
    "collision_type             object\n",
    "incident_severity          object\n",
    "authorities_contacted      object\n",
    "incident_state             object\n",
    "incident_city              object\n",
    "incident_location          object\n",
    "property_damage            category\n",
    "bodily_injuries            object\n",
    "witnesses                  object\n",
    "police_report_available    category\n",
    "auto_year                  object\n",
    "auto_make                  object\n",
    "auto_model                 object\n",
    "fraud_reported                category\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQ0680R6NXTz"
   },
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "##Binary and Small numbers of categories\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "label_col = ['policy_state', 'policy_csl', 'insured_sex', 'property_damage', 'police_report_available', 'fraud_reported']\n",
    "for col in label_col:\n",
    "    df_ICFD[col] = le.fit_transform(df_ICFD[col])\n",
    "\n",
    "df_ICFD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5JLln05N-xia"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "num_df = df_ICFD.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "fige, ax = plt.subplots(figsize=(25, 20))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "sns.heatmap(num_df.corr(), annot=True,vmax = .3, cmap=cmap, ax=ax)\n",
    "\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4EgvqVp_4Qu"
   },
   "source": [
    "Here first correlation between Age and months_as_customers is have high correlation  : 0.92\n",
    "\n",
    "as well as  :          \n",
    "total_claim_amount vs  injury_clime (0.81)\n",
    "total_claim_amount vs property_clime (0.81)\n",
    "total_claim_amount vs Vehicle_clime(0.98)\n",
    "injury_clime vs Vehicle_clime (0.72)\n",
    "property_clime vs Vehicle_clime(0.73)\n",
    "  between this four have high positive  correlation .\n",
    "\n",
    "property_clime vs injury_clime (0.56)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8Gpk4XyNz3K"
   },
   "outputs": [],
   "source": [
    "#Categorical columns with many Unique values\n",
    "#policy_number, insured_zip , incident_location ,auto_make,auto_model\n",
    "\n",
    "#Unique values : 999,994,999,14,39\n",
    "\n",
    "def frequency_encoding(df_ICFD,column):\n",
    "  freq= df_ICFD[column].value_counts()\n",
    "  df_ICFD[column + '_freq']=df_ICFD[column].map(freq)\n",
    "  df_ICFD.drop(column,axis=1,inplace=True)\n",
    "\n",
    "frequency_col = ['policy_number','insured_zip', 'incident_location', 'auto_make', 'auto_model']\n",
    "for col in frequency_col:\n",
    "  frequency_encoding(df_ICFD,col)\n",
    "\n",
    "df_ICFD.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EfKyvJtCQtn0"
   },
   "outputs": [],
   "source": [
    "#One-hot Encoding or label encoding\n",
    "one_hot_ecode_col  = ['insured_education_level','insured_occupation','insured_hobbies','insured_relationship','incident_type', 'collision_type', 'incident_severity', 'authorities_contacted', 'incident_state', 'incident_city']\n",
    "df_ICFD = pd.get_dummies(df_ICFD, columns=one_hot_ecode_col, drop_first=True)\n",
    "df_ICFD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQcGDJhbSv7m"
   },
   "outputs": [],
   "source": [
    "df_ICFD.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3aMfZEFYK4Q_"
   },
   "outputs": [],
   "source": [
    "df_ICFD.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYlM4chpUos4"
   },
   "outputs": [],
   "source": [
    "df_ICFD.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6zAZM35RrsK"
   },
   "outputs": [],
   "source": [
    "df_ICFD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3nAcCxsVmzV"
   },
   "outputs": [],
   "source": [
    "bool_val = df_ICFD.select_dtypes(include='bool').columns\n",
    "df_ICFD[bool_val] = df_ICFD[bool_val].astype(int)\n",
    "df_ICFD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8EOMLzu9VPxw"
   },
   "outputs": [],
   "source": [
    "print(df_ICFD.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frXUwPgkVUS7"
   },
   "outputs": [],
   "source": [
    "df_ICFD.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "di4R-2-tWdaW"
   },
   "outputs": [],
   "source": [
    "object_cols = df_ICFD.select_dtypes(include=['object']).columns\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kuXis8fMpMFA"
   },
   "outputs": [],
   "source": [
    "df_ICFD['fraud_reported'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGau7kQcpohW"
   },
   "outputs": [],
   "source": [
    "#Skewness\n",
    "skewness = df_ICFD.skew()\n",
    "skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8Jri8yRD052"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGUEH4BdD44O"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DyjlSCXy7Rq5"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fige, ax = plt.subplots(figsize=(25, 20))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "sns.heatmap(df_ICFD.corr(), annot=True,vmax = .3, cmap=cmap, ax=ax)\n",
    "\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtI75T-RqZR1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "for feature in df_ICFD.columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(df_ICFD[feature], kde=True)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tdP-tJear_tw"
   },
   "outputs": [],
   "source": [
    "#for Box plot\n",
    "\n",
    "for feature in df_ICFD.columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(x=df_ICFD[feature])\n",
    "    plt.title(f'Box Plot of {feature}')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dw6FacnDD6hc"
   },
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "sns.countplot(x='fraud_reported', data=df_ICFD)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcH-R8sMFFgl"
   },
   "outputs": [],
   "source": [
    "#Here I can find more hobbies in Insured ,I need to melt it in insured columns\n",
    "\n",
    "df_melted = pd.melt(df_ICFD, id_vars=['fraud_reported'], value_vars=[col for col in df_ICFD.columns if 'insured_hobbies_' in col ],var_name = 'insured_hobbies', value_name='is_hobbies')\n",
    "df_melted = df_melted[df_melted['is_hobbies'] == 1]\n",
    "df_melted.head()\n",
    "\n",
    "f,ax = plt.subplots(figsize=(10, 8))\n",
    "sns.countplot(x='fraud_reported', hue = 'insured_hobbies', data=df_melted)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6RkgPQjh6bX"
   },
   "outputs": [],
   "source": [
    "#Second time correlation test\n",
    "\n",
    "corr = df_ICFD.corr()\n",
    "corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_gXyBud_wYut"
   },
   "outputs": [],
   "source": [
    "!pip install lightgbm==3.3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6E-_40k9ijFD"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "X = df_ICFD.drop('fraud_reported', axis=1)\n",
    "y = df_ICFD['fraud_reported']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHW8wHbqinRV"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "def lgb_f1_score(y_hat, df_ICFD):\n",
    "    y_true = df_ICFD.get_label()\n",
    "    y_hat = np.where(y_hat < 0.5, 0, 1)  # Convert probabilities to 0/1\n",
    "    return 'f1',  f1_score(y_true, y_hat),True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pu-Mviq_lUvS"
   },
   "outputs": [],
   "source": [
    "def r_lgb(X_train, X_test, y_train, y_test, test_data):\n",
    "  params = {\n",
    "      'objective': 'binary',\n",
    "      'metric': 'f1',\n",
    "      'boosting': 'gbdt',\n",
    "      'num_leaves': 3,\n",
    "      'learning_rate': 0.05,\n",
    "      'feature_fraction': 0.9,\n",
    "      #'bagging_fraction': 0.8,\n",
    "      'bagging_freq': 5,\n",
    "      'verbose': 0,\n",
    "      'num_iterations': 200,\n",
    "      'n_jobs': -1,\n",
    "      'random_state': 42,\n",
    "      'lambda_l1': 0.1,\n",
    "      'lambda_l2': 0.1,\n",
    "      'min_data_in_leaf': 10,\n",
    "      'max_depth': -1,\n",
    "      'min_child_weight': 0.001,\n",
    "      'reg_alpha': 0.5,\n",
    "      'reg_lambda': 0.5,\n",
    "      'min_split_gain': 0.0222415,\n",
    "      'subsample': 1.0,\n",
    "      'subsample_freq': 1,\n",
    "      'force_row_wise': True\n",
    "  }\n",
    "\n",
    "  X_train.columns = [col.replace(' ','_') for col in X_train.columns]\n",
    "  X_test.columns = [col.replace(' ','_') for col in X_test.columns]\n",
    "\n",
    "  # Create LightGBM datasets\n",
    "  lgtrain = lgb.Dataset(X_train, label=y_train)\n",
    "  lgval = lgb.Dataset(X_test, label=y_test)\n",
    "  evals_result = {}\n",
    "  model = lgb.train(params, lgtrain, 5000,\n",
    "                      valid_sets=[lgtrain, lgval],\n",
    "                      early_stopping_rounds=100,\n",
    "                      verbose_eval=100,\n",
    "                      evals_result=evals_result,feval = lgb_f1_score)\n",
    "  pred_test_y = model.predict(test_data,num_iteration = model.best_iteration)\n",
    "  return pred_test_y, model, evals_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZhggY2csrPp"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "pred_test, model,evals_result = r_lgb(X_train, X_test, y_train, y_test, X_test)\n",
    "\n",
    "#Convert probabilities to 0/1 predictions\n",
    "pred_test_binary = np.where(pred_test < 0.5, 0, 1)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, pred_test_binary)\n",
    "\n",
    "# Print the F1 score\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"LightGBM Model Performance\")\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, pred_test_binary)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_kb5kE7zUAH"
   },
   "outputs": [],
   "source": [
    "roc_auc_score(y_test,pred_test)\n",
    "\n",
    "#roc_auc_score(y_test,pred_test_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Ps2A8gj5Nwx"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, pred_test)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M1j1kqwT6Mhp"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, pred_test)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7vfq6cn7gyd"
   },
   "outputs": [],
   "source": [
    "\n",
    "print('plot features')\n",
    "\n",
    "ax = lgb.plot_importance(model, max_num_features=20, height=0.5)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LvkKbmbU-Oj2"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XObarCts-rke"
   },
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "          'svc':SVC(),\n",
    "          'rfc':RandomForestClassifier(),\n",
    "          'knc':KNeighborsClassifier(),\n",
    "          'gau':GaussianNB(),\n",
    "          'dtc' : DecisionTreeClassifier(),\n",
    "          'abc' : AdaBoostClassifier(),\n",
    "          'grd':GradientBoostingClassifier(),\n",
    "          'bagg':BaggingClassifier(),\n",
    "          'xgb':xgb.XGBClassifier(),\n",
    "          'lgb':lgb.LGBMClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0CXYF_G-ZTP"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "#train AND evalate classifiers for primary fuel predection\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "  classifier.fit(X_train,y_train)\n",
    "  y_pred = classifier.predict(X_test)\n",
    "  accuracy = accuracy_score(y_test,y_pred)\n",
    "  print(f'{name} : {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKrVYe5S-5PY"
   },
   "outputs": [],
   "source": [
    "GaussianNB_model = GaussianNB()\n",
    "GaussianNB_model.fit(X_train,y_train)\n",
    "y_pred = GaussianNB_model.predict(X_test)\n",
    "\n",
    "Accuracy = accuracy_score(y_test,y_pred)\n",
    "print(f'Accuracy : {Accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHRxWQAI_Ath"
   },
   "outputs": [],
   "source": [
    "GaussianNB_model = GaussianNB()\n",
    "GaussianNB_model.fit(X_train,y_train)\n",
    "y_pred = GaussianNB_model.predict(X_test)\n",
    "\n",
    "Accuracy = accuracy_score(y_test,y_pred)\n",
    "print(f'Accuracy : {Accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axBhHSjK_PlM"
   },
   "outputs": [],
   "source": [
    "SVC_model = SVC()\n",
    "SVC_model.fit(X_train,y_train)\n",
    "y_pred = SVC_model.predict(X_test)\n",
    "\n",
    "Accuracy = accuracy_score(y_test,y_pred)\n",
    "print(f'Accuracy SVC_model: {Accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iH_CgZlz_beO"
   },
   "outputs": [],
   "source": [
    "Ada_model = AdaBoostClassifier()\n",
    "Ada_model.fit(X_train,y_train)\n",
    "y_pred = Ada_model.predict(X_test)\n",
    "\n",
    "Accuracy = accuracy_score(y_test,y_pred)\n",
    "print(f'Accuracy Ada_model: {Accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-cTuE6-_rh7"
   },
   "outputs": [],
   "source": [
    "Gradient_model  = GradientBoostingClassifier()\n",
    "Gradient_model.fit(X_train,y_train)\n",
    "y_pred = Gradient_model.predict(X_test)\n",
    "\n",
    "Accuracy = accuracy_score(y_test,y_pred)\n",
    "print(f'Accuracy Gradient_model: {Accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRTpHP-VAHLs"
   },
   "source": [
    "##Best model\n",
    "we can consider **GradientBoostingClassifier** is the best model when I compare with other models in above list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfI06eMs_7yL"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'Gradient_model.pkl'\n",
    "pickle.dump(Gradient_model, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPKwj/5FfXKVdVwNwtgVf+8",
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
