{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFNineegvwZh"
   },
   "source": [
    "**Zomato Restaurant Project Description :**\n",
    "Zomato Data Analysis is one of the most useful analysis for foodies who want to taste the best\n",
    "\n",
    "cuisines of every part of the world which lies in their budget. This analysis is also for those who want to find the value for money restaurants in various parts of the country for the cuisines.\n",
    "\n",
    "Additionally, this analysis caters the needs of people who are striving to get the best cuisine of the country and which locality of that country serves that cuisines with maximum number of restaurants.\n",
    "\n",
    "\n",
    "**Data Storage:**\n",
    "This problem statement contains two datasets- Zomato.csv and country_code.csv.\n",
    "Country_code.csv contains two variables:\n",
    "\n",
    "##• Country code\n",
    "##• Country name\n",
    "\n",
    "The collected data has been stored in the Comma Separated Value file Zomato.csv.\n",
    "\n",
    "  Each restaurant in the dataset is uniquely identified by its Restaurant Id. **Every Restaurant contains the following variables:**\n",
    "\n",
    "• Restaurant Id: Unique id of every restaurant across various cities of the world\n",
    "\n",
    "• Restaurant Name: Name of the restaurant\n",
    "\n",
    "• Country Code: Country in which restaurant is located\n",
    "\n",
    "• City: City in which restaurant is located\n",
    "\n",
    "• Address: Address of the restaurant\n",
    "\n",
    "• Locality: Location in the city\n",
    "\n",
    "• Locality Verbose: Detailed description of the locality\n",
    "\n",
    "• Longitude: Longitude coordinate of the restaurant&#39;s location\n",
    "\n",
    "• Latitude: Latitude coordinate of the restaurant&#39;s location\n",
    "\n",
    "• Cuisines: Cuisines offered by the restaurant\n",
    "\n",
    "• Average Cost for two: Cost for two people in different currencies\n",
    "\n",
    "• Currency: Currency of the country\n",
    "\n",
    "• Has Table booking: yes/no\n",
    "\n",
    "• Has Online delivery: yes/ no\n",
    "\n",
    "• Is delivering: yes/ no\n",
    "\n",
    "• Switch to order menu: yes/no\n",
    "\n",
    "• Price range: range of price of food\n",
    "\n",
    "• Aggregate Rating: Average rating out of 5\n",
    "\n",
    "• Rating color: depending upon the average rating color\n",
    "\n",
    "• Rating text: text on the basis of rating of rating\n",
    "\n",
    "• Votes: Number of ratings casted by people\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4DT97d-v5-P"
   },
   "source": [
    "\n",
    "**Problem statement :**\n",
    "\n",
    "In this dataset predict 2 things –\n",
    "1) Average Cost for two\n",
    "2) Price range\n",
    "\n",
    "\n",
    "Hint : Use pandas methods to combine all the datasets and then start working on this project.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYiTXmZ-wBFu"
   },
   "source": [
    "**Dataset Link-**\n",
    "\n",
    "•\thttps://github.com/FlipRoboTechnologies/ML_-Datasets/blob/main/Z_Restaurant/Country-Code.xlsx\n",
    "\n",
    "•\thttps://raw.githubusercontent.com/FlipRoboTechnologies/ML_-Datasets/main/Z_Restaurant/zomato.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DlPwwHzQv487"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7IXVPLcuy2FX"
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m df1_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/FlipRoboTechnologies/ML_-Datasets/main/Z_Restaurant/zomato.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m df2_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/FlipRoboTechnologies/ML_-Datasets/main/Z_Restaurant/Country-Code.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m df1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(df1_url,encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin-1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(df2_url)\n\u001b[0;32m      6\u001b[0m df1\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:713\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    710\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[0;32m    712\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[1;32m--> 713\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m _get_filepath_or_buffer(\n\u001b[0;32m    714\u001b[0m     path_or_buf,\n\u001b[0;32m    715\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m    716\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    717\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    718\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    719\u001b[0m )\n\u001b[0;32m    721\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[0;32m    722\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:363\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[0;32m    362\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urlopen(req_info) \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[0;32m    364\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    366\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:265\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m meth(req, response)\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m, request, response, code, msg, hdrs)\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "#Data Loading and Merging the data\n",
    "df1_url = 'https://raw.githubusercontent.com/FlipRoboTechnologies/ML_-Datasets/main/Z_Restaurant/zomato.csv'\n",
    "df2_url = 'https://raw.githubusercontent.com/FlipRoboTechnologies/ML_-Datasets/main/Z_Restaurant/Country-Code.xlsx'\n",
    "df1 = pd.read_csv(df1_url,encoding='latin-1')\n",
    "df2 = pd.read_excel(df2_url)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6WLQTzAfLRW"
   },
   "outputs": [],
   "source": [
    "print(df1.columns)\n",
    "\n",
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYfjJHIH4xJO"
   },
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EEbvhUl942RW"
   },
   "outputs": [],
   "source": [
    "df_zomoto = pd.merge(df1,df2,on='Country Code',how='left')\n",
    "df_zomoto.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuEnFYfng4kZ"
   },
   "source": [
    "**Exploratory Data Analysis (EDA):**\n",
    "\n",
    "Perform EDA to understand the distribution of features, handle missing values, and identify correlations.\n",
    "\n",
    "Visualize data to find insights about the restaurant locations, cuisines, and pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lv7gJPjdhMxY"
   },
   "outputs": [],
   "source": [
    "df_zomoto.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nv4y0EPPhnAn"
   },
   "outputs": [],
   "source": [
    "df_zomoto.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lt8rlJDRECXu"
   },
   "outputs": [],
   "source": [
    "df_zomoto.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_jTu--_EBLS"
   },
   "source": [
    "##Exploratory Data analysis (EDA)\n",
    "We can see ,there is no missing values in the columns in our dataset\n",
    "we will check again if  any duplicates and missing values  in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQLbXWLYEAuu"
   },
   "outputs": [],
   "source": [
    "df_zomoto.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RvflP34PEvPY"
   },
   "outputs": [],
   "source": [
    "df_zomoto.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VoZ4HC5cEyc2"
   },
   "outputs": [],
   "source": [
    "#Distrubution of average Cost for two\n",
    "#Distplot\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.distplot(df_zomoto['Average Cost for two'], bins=50,kde =True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F69ciP86FPW9"
   },
   "outputs": [],
   "source": [
    "#Histplot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(df_zomoto['Average Cost for two'], bins=50,kde =True)\n",
    "plt.title('Average Cost for two')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-6Eyni1Fx4T"
   },
   "outputs": [],
   "source": [
    "#Correlation heatmap\n",
    "plt.figure(figsize=(10,6))\n",
    "corr = df_zomoto.select_dtypes(include=[np.number]).corr()\n",
    "sns.heatmap(corr,annot=True,cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZGDU6OyJ_I8"
   },
   "source": [
    "As per the above heatmap , there is no significant correlation between 'Average Cost for two ' and other fatures\n",
    "\n",
    "The correlation are relatively low  ,with the highest being a weak positive correlation of 0.075 with **prince Range**\n",
    "\n",
    "**Price Range :** Price range have some moderate positive correlaion with '**aggregate rating**  and **Votes** i.e 0.44 and 0.31\n",
    "this suggestes that highest priced restartents tend to have better ratings and more votes\n",
    "\n",
    "aggregate rating vs votes : 0.31\n",
    "aggregate rating vs Price range 0.44\n",
    "price range vs votes : 0.31\n",
    "\n",
    "**Country Code**  has a moderate negitive correlation with 'longitude'(-0.70).\n",
    "\n",
    "this likely reflects geograpical clustering of the data,where certain country codes are associated with specific longitude ranges\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2vLT9lGzj8w"
   },
   "outputs": [],
   "source": [
    "df_zomoto.columns\n",
    "print(df_zomoto.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MgB945WuyZvc"
   },
   "outputs": [],
   "source": [
    "skewness = df_zomoto.select_dtypes(include=[np.number]).skew()\n",
    "print(skewness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHxDFmVP0Yrx"
   },
   "source": [
    "#Skewness result is :\n",
    "\n",
    "Restaurant ID 0.061570\n",
    "Country Code 3.043965\n",
    " Longitude -2.807328\n",
    " Latitude -3.081635\n",
    "  Average Cost for two 35.477915\n",
    "   Price range 0.889618\n",
    "   Aggregate rating -0.954130\n",
    "   Votes 8.807637"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJJbz25G1ivL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Gk3g4pN0qgZ"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler,PowerTransformer\n",
    "\n",
    "num_cols = ['Restaurant ID','Country Code','Longitude','Latitude','Average Cost for two','Price range','Aggregate rating','Votes']\n",
    "\n",
    "df_zomoto['Average Cost for two'] = np.log1p(df_zomoto['Average Cost for two'])\n",
    "df_zomoto['Votes'] = np.log1p(df_zomoto['Votes'])\n",
    "\n",
    "#Power transfer method\n",
    "power_transformer = PowerTransformer(method='yeo-johnson')\n",
    "df_zomoto[num_cols] = power_transformer.fit_transform(df_zomoto[num_cols])\n",
    "\n",
    "#StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_zomoto[num_cols] = scaler.fit_transform(df_zomoto[num_cols])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SA56SF0d2EOH"
   },
   "outputs": [],
   "source": [
    "df_zomoto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWia7DHc2V2G"
   },
   "outputs": [],
   "source": [
    "#second time\n",
    "skewness = df_zomoto.select_dtypes(include=[np.number]).skew()\n",
    "print(skewness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ah7IPH1Z3mHq"
   },
   "outputs": [],
   "source": [
    "high_skew_col = ['Country Code', 'Longitude','Latitude']\n",
    "\n",
    "for col in high_skew_col:\n",
    "  df_zomoto[col] = power_transformer.fit_transform(df_zomoto[[col]])\n",
    "\n",
    "  scaler1 = StandardScaler()\n",
    "  df_zomoto[col] = scaler1.fit_transform(df_zomoto[[col]])\n",
    "\n",
    "skewness = df_zomoto.select_dtypes(include=[np.number]).skew()\n",
    "print(skewness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPcNSA8l1BpQ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdOjfx4Z0FpK"
   },
   "source": [
    "Before Proceeding with model deployment ,we need to encode the catagerical features into numerical values\n",
    "\n",
    "There are ⁉\n",
    "**Label Encoding:** Assigns each unique category a different integer. This can be used when there is an ordinal relationship between categories\n",
    "\n",
    "**One-Hot Encoding:** Creates binary columns for each category. This is used when there is no ordinal relationship between categories\n",
    "\n",
    "**we will apply the following encoding strategies:**\n",
    "**Label Encoding** for columns where ordinal relationship might exist or where we have a small number of unique values\n",
    "**One-Hot Encoding** for columns with no ordinal relationship and a manageable number of unique values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4UJ2kNVa0_Yr"
   },
   "outputs": [],
   "source": [
    "label_cols = ['Has Table booking','Has Online delivery','Is delivering now','Switch to order menu','Rating color','Rating text']\n",
    "onehot_cols = ['Country','City','Address','Locality','Locality Verbose','Cuisines','Currency','Restaurant Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLFoZ9T71qYy"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create a LabelEncoder object\n",
    "le_encode = LabelEncoder()\n",
    "for col in label_cols:\n",
    "  df_zomoto[col] = le_encode.fit_transform(df_zomoto[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1CjLEp714vQ"
   },
   "outputs": [],
   "source": [
    "#one hot encoder\n",
    "df_zomoto = pd.get_dummies(df_zomoto,columns=onehot_cols)\n",
    "df_zomoto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UvTPRC8Y2D9z"
   },
   "outputs": [],
   "source": [
    "#after encoding the dataset\n",
    "df_zomoto.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LLpB420IMM1q"
   },
   "outputs": [],
   "source": [
    "df_zomoto.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RiwY4xZxLGvx"
   },
   "outputs": [],
   "source": [
    "#Encoding False and True Values\n",
    "boolean_col = ['Restaurant Name_feel ALIVE','Restaurant Name_hug!',\n",
    "       'Restaurant Name_iGNiTE', 'Restaurant Name_iKitchen',\n",
    "       'Restaurant Name_sketch Gallery', 'Restaurant Name_t Lounge by Dilmah',\n",
    "       'Restaurant Name_tashas', 'Restaurant Name_wagamama',\n",
    "       'Restaurant Name_{Niche} - Cafe & Bar',\n",
    "       'Restaurant Name_íukuraÛôa SofrasÛ±']\n",
    "for col in boolean_col:\n",
    "  df_zomoto[col] = df_zomoto[col].replace({False:0,True:1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4GJXm8yM6bl"
   },
   "outputs": [],
   "source": [
    "print(df_zomoto.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YnLwXDfNG0T"
   },
   "outputs": [],
   "source": [
    "print(df_zomoto.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWqZ7nAPvJwI"
   },
   "source": [
    "##Predicting average cost for two\n",
    "\n",
    "we'll use regression models such as linear regression,desision trees and random forests to predict average cost for two\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6PWy1emwMjN"
   },
   "outputs": [],
   "source": [
    "!pip install xgboost\n",
    "!pip install nvidia-cublas-cu12==12.1.3.1\n",
    "!pip install nvidia-cuda-cupti-cu12==12.1.105\n",
    "!pip install nvidia-cuda-runtime-cu12==12.1.105\n",
    "!pip install nvidia-cudnn-cu12==8.9.2.26\n",
    "!pip install nvidia-cufft-cu12==11.0.2.54\n",
    "!pip install nvidia-curand-cu12==10.3.2.106\n",
    "!pip install nvidia-cusolver-cu12==11.4.5.107\n",
    "!pip install nvidia-cusparse-cu12==12.1.0.106\n",
    "!pip install nvidia-nvtx-cu12==12.1.105\n",
    "!pip install nvidia-nccl-cu12==2.20.5\n",
    "!pip install nvidia-nsight-cu12==2.20.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yTGnORnlEmr"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor,GradientBoostingClassifier,GradientBoostingRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,f1_score,precision_score,recall_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azwn4deMzVvt"
   },
   "outputs": [],
   "source": [
    "#feature and target variables\n",
    "features = df_zomoto.drop(['Average Cost for two','Price range'],axis=1)\n",
    "target_cost = df_zomoto['Average Cost for two']\n",
    "target_price = df_zomoto['Price range']\n",
    "\n",
    "#Train and test split\n",
    "X_train_cost,X_test_cost,y_train_cost,y_test_cost = train_test_split(features,target_cost,test_size=0.2,random_state=42)\n",
    "X_train_price,X_test_price,y_train_price,y_test_price = train_test_split(features,target_price,test_size=0.2,random_state=42)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_cost = scaler.fit_transform(X_train_cost)\n",
    "X_test_cost = scaler.transform(X_test_cost)\n",
    "\n",
    "#applying PCA to reduce the number of features\n",
    "PCA_cost = PCA(n_components=10)\n",
    "X_train_cost = PCA_cost.fit_transform(X_train_cost)\n",
    "X_test_cost = PCA_cost.transform(X_test_cost)\n",
    "\n",
    "rf_model_cost = RandomForestRegressor(n_estimators=100,random_state=42)\n",
    "rf_model_cost.fit(X_train_cost,y_train_cost)\n",
    "y_pred_rf_cost = rf_model_cost.predict(X_test_cost)\n",
    "\n",
    "rmse_cose = mean_squared_error(y_test_cost,y_pred_rf_cost,squared=False)\n",
    "print(\"RandomForestRegressor - Root mean squared error(Cost):\",rmse_cose)\n",
    "print(\"RandomForestRegressor -Mean absolute error(Cost):\",mean_absolute_error(y_test_cost,y_pred_rf_cost))\n",
    "print(\"RandomForestRegressor -R2 score(Cost):\",r2_score(y_test_cost,y_pred_rf_cost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UmmNpBMz9G0F"
   },
   "outputs": [],
   "source": [
    "#RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
    "\n",
    "#Price range\n",
    "# Price using classifier\n",
    "X_train_price = scaler.fit_transform(X_train_price)\n",
    "X_test_price = scaler.transform(X_test_price)\n",
    "\n",
    "#applying PCA to reduce the number of features\n",
    "PCA_price = PCA(n_components=10)\n",
    "X_train_price = PCA_price.fit_transform(X_train_price)\n",
    "X_test_price = PCA_price.transform(X_test_price)\n",
    "\n",
    "\n",
    "rfr_model_price = RandomForestRegressor(n_estimators=100,random_state=42)\n",
    "rfr_model_price.fit(X_train_price,y_train_price)\n",
    "y_pred_rfr_price = rfr_model_price.predict(X_test_price)\n",
    "\n",
    "rmse_price = mean_squared_error(y_test_price,y_pred_rfr_price,squared=False)\n",
    "print(\"RandomForestRegressor- Root mean squared error(Price):\",rmse_price)\n",
    "print(\"RandomForestRegressor-Mean absolute error(Price):\",mean_absolute_error(y_test_price,y_pred_rfr_price))\n",
    "print(\"RandomForestRegressor-R2 score(Price):\",r2_score(y_test_price,y_pred_rfr_price))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRRhh6xEJxsr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSVCacLZjRUT"
   },
   "outputs": [],
   "source": [
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100,150],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'learning_rate' : [0.01,0.1,1]\n",
    "}\n",
    "\n",
    "XGB_model_cost = XGBRegressor()\n",
    "XGB_GridSearch_cost = GridSearchCV(estimator=XGB_model_cost, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "XGB_GridSearch_cost.fit(X_train_cost,y_train_cost)\n",
    "\n",
    "best_params = XGB_GridSearch_cost.best_params_\n",
    "print(\"Best parameters:\",best_params)\n",
    "\n",
    "best_model = XGBRegressor(**best_params)\n",
    "best_model.fit(X_train_cost,y_train_cost)\n",
    "\n",
    "y_pred_XGB_cost = best_model.predict(X_test_cost)\n",
    "\n",
    "\n",
    "rmse = mean_squared_error(y_test_cost,y_pred_XGB_cost,squared=False)\n",
    "print(\"XGBRegressor-Root mean squared error(Cost):\",rmse)\n",
    "print(\"XGBRegressor-Mean absolute error(Cost):\",mean_absolute_error(y_test_cost,y_pred_XGB_cost))\n",
    "print(\"XGBRegressor-R2 score(Cost):\",r2_score(y_test_cost,y_pred_XGB_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcHzBEXzRdOL"
   },
   "source": [
    "Best parameters: {'learning_rate': 0.1, 'max_depth': 4, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 150}\n",
    "XGBRegressor-Root mean squared error(Cost): 0.5976602952205758\n",
    "XGBRegressor-Mean absolute error(Cost): 0.43896865236483845\n",
    "XGBRegressor-R2 score(Cost): 0.6368710783978982\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I33DEQ_uSLvg"
   },
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "ploy_cost = PolynomialFeatures(degree=2,include_bias=False)\n",
    "X_train_cost_poly = ploy_cost.fit_transform(X_train_cost)\n",
    "X_test_cost_poly = ploy_cost.transform(X_test_cost)\n",
    "\n",
    "ploy_price = PolynomialFeatures(degree=2,include_bias=False)\n",
    "X_train_price_poly = ploy_price.fit_transform(X_train_price)\n",
    "X_test_price_poly = ploy_price.transform(X_test_price)\n",
    "\n",
    "\n",
    "best_model.fit(X_train_cost_poly,y_train_cost)\n",
    "y_pred_XGB_cost = best_model.predict(X_test_cost_poly)\n",
    "\n",
    "\n",
    "rmse = mean_squared_error(y_test_cost,y_pred_XGB_cost,squared=False)\n",
    "print(\"PolynomialFeatures-Root mean squared error(Cost):\",rmse)\n",
    "print(\"PolynomialFeaturesMean - absolute error(Cost):\",mean_absolute_error(y_test_cost,y_pred_XGB_cost))\n",
    "print(\"PolynomialFeatures-R2 score(Cost):\",r2_score(y_test_cost,y_pred_XGB_cost))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "cell_execution_strategy": "setup",
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
